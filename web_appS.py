import streamlit as st
import pandas as pd
import requests
import time
import io
import base64
from datetime import datetime

# ç¶²é åŸºæœ¬è¨­å®š
st.set_page_config(page_title="ä¸€æ‰‹å•†å“åº«å­˜æŠ“å–å·¥å…·", page_icon="ğŸ“¦")

st.title("ğŸ“¦ ä¸€æ‰‹å•†å“åº«å­˜æŠ“å–ç³»çµ± (æ–·é»çºŒå‚³ç‰ˆ)")
st.markdown("---")

# åˆå§‹åŒ– Session State (ç”¨æ–¼æ–·ç·šä¿å…¨)
if 'collected_data' not in st.session_state:
    st.session_state.collected_data = [] # å„²å­˜æŠ“åˆ°çš„è³‡æ–™è¡Œ
if 'processed_gids' not in st.session_state:
    st.session_state.processed_gids = set() # å„²å­˜å·²å®Œæˆçš„ ID

# å´é‚Šæ¬„è¨­å®šå€
st.sidebar.header("ğŸ“‹ ä»»å‹™åƒæ•¸è¨­å®š")
start_date = st.sidebar.date_input("é–‹å§‹æ—¥æœŸ", datetime(2025, 12, 29))
end_date = st.sidebar.date_input("çµæŸæ—¥æœŸ", datetime(2025, 12, 29))
uploaded_file = st.sidebar.file_uploader("ä¸Šå‚³ä¾›æ‡‰å•†åå–® (SHOP.xls)", type=["xls", "xlsx", "csv"])

if st.sidebar.button("ğŸ—‘ï¸ æ¸…ç©ºå¿«å–/é‡æ–°é–‹å§‹"):
    st.session_state.collected_data = []
    st.session_state.processed_gids = set()
    st.rerun()

# çˆ¬èŸ²æ ¸å¿ƒé‚è¼¯
class YishouWebScraper:
    def __init__(self, start_date, end_date):
        self.start_datetime = datetime.combine(start_date, datetime.min.time())
        self.end_datetime = datetime.combine(end_date, datetime.max.time())
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 MicroMessenger/7.0.20.1781",
            "Content-Type": "application/x-www-form-urlencoded",
            "Referer": "https://servicewechat.com/wx6309080f/9129/page-frame.html",
            "Host": "api.yishouapp.com"
        }

    def get_valid_goods_ids(self, supplier_id):
        valid_ids = set()
        try: s_id_str = str(int(float(supplier_id)))
        except: return []
        
        page = 1
        while True:
            payload = {"supplier_id": s_id_str, "pageindex": str(page), "pagesize": "20", "version": "3.9.8", "plat_type": "wx", "sort": "5", "t_t": str(int(time.time() * 1000))}
            try:
                res = requests.post("https://api.yishouapp.com/supplier/supplier_detail_by_supplier_id", headers=self.headers, data=payload, timeout=15).json()
                goods = res.get("data", {}).get("goods", [])
                if not goods: break
                for g in goods:
                    sale_time = g.get("first_sale_time", "")
                    if not sale_time: continue
                    item_date = datetime.strptime(sale_time, "%Y%m%d")
                    if self.start_datetime <= item_date <= self.end_datetime:
                        valid_ids.add(str(g["goods_id"]))
                    elif item_date < self.start_datetime: return list(valid_ids)
                page += 1
                time.sleep(0.3)
            except Exception as e:
                st.warning(f"æƒæä¾›æ‡‰å•† {supplier_id} æ™‚ç™¼ç”Ÿç¶²è·¯æŠ–å‹•ï¼Œå˜—è©¦è·³éè©²é ã€‚")
                break
        return list(valid_ids)

    def get_sku_details(self, goods_id):
        payload = {"goods_id": str(goods_id), "version": "3.9.8", "plat_type": "wx"}
        try:
            res = requests.post("https://api.yishouapp.com/goods/get_goods_info_v2", headers=self.headers, data=payload, timeout=15).json()
            data = res.get("data", {})
            main_img = data.get("goods_img", "")
            all_imgs = "|".join(data.get("imgs", []))
            
            rows = []
            for attr in data.get("attribute", []):
                color = attr.get("color", "N/A")
                for item in attr.get("item", []):
                    stock_val = int(item.get('stock', 0))
                    if stock_val <= 1: continue 
                    rows.append({
                        "provider_name": data.get("supplier_name", ""),
                        "brand_name": data.get("brand_name", ""),
                        "first_sale_time": data.get("first_sale_time", ""),
                        "goods_id": goods_id,
                        "goods_name": data.get("goods_name", ""),
                        "code": data.get("goods_sn", ""),
                        "goods_img": main_img,
                        "price": data.get("shop_price", ""),
                        "sku": item.get("sku", "N/A"),
                        "color": color,
                        "size": item.get("size", "N/A"),
                        "imgs": all_imgs
                    })
            return rows
        except Exception as e:
            return None # å›å‚³ None ä»£è¡¨å¤±æ•—

# è‡ªå‹•ä¸‹è¼‰å‡½å¼
def f_auto_download(df):
    csv = df.to_csv(index=False, encoding='utf-8-sig')
    b64 = base64.b64encode(csv.encode('utf-8-sig')).decode()
    file_name = f"ä¸€æ‰‹æŠ“å–_è£œæŠ“ç‰ˆ_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
    js = f"""
    <a id="download_link" href="data:file/csv;base64,{b64}" download="{file_name}" style="display:none;">Download</a>
    <script>document.getElementById('download_link').click();</script>
    """
    st.components.v1.html(js, height=0)

# åŸ·è¡Œé‚è¼¯
if uploaded_file and st.sidebar.button("ğŸš€ é–‹å§‹åŸ·è¡Œ(æ”¯æ´çºŒå‚³)"):
    try:
        file_bytes = uploaded_file.read()
        try: df = pd.read_excel(io.BytesIO(file_bytes))
        except:
            try: df = pd.read_csv(io.BytesIO(file_bytes), encoding='utf-8-sig')
            except: df = pd.read_csv(io.BytesIO(file_bytes), encoding='cp950')
        df.columns = df.columns.astype(str).str.strip().str.lower()
        s_ids = pd.to_numeric(df['supplier_id'], errors='coerce').dropna().astype(int).unique()
    except Exception as e:
        st.error(f"æª”æ¡ˆè§£æå¤±æ•—: {e}"); st.stop()

    scraper = YishouWebScraper(start_date, end_date)
    status_bar = st.empty()
    
    # ç¬¬ä¸€æ­¥ï¼šæƒæ ID
    all_gids = set()
    for i, sid in enumerate(s_ids):
        status_bar.info(f"ğŸ” æ­£åœ¨æƒæä¾›æ‡‰å•†åˆ—è¡¨... ({i+1}/{len(s_ids)})")
        all_gids.update(scraper.get_valid_goods_ids(sid))
    
    # ç¬¬äºŒæ­¥ï¼šéæ¿¾æ‰ã€Œå·²ç¶“è™•ç†éã€çš„ ID
    total_list = list(all_gids)
    remaining_list = [gid for gid in total_list if gid not in st.session_state.processed_gids]
    
    if not remaining_list:
        st.success("âœ¨ æ‰€æœ‰å•†å“å·²è™•ç†å®Œç•¢ï¼")
    else:
        st.warning(f"ğŸ“Š ç¸½å…± {len(total_list)} å€‹å•†å“ï¼Œå°šé¤˜ {len(remaining_list)} å€‹å¾…æŠ“å–ã€‚")
        progress_bar = st.progress(0)
        
        for j, gid in enumerate(remaining_list, 1):
            status_bar.text(f"æ­£åœ¨æŠ“å–è©³æƒ…: {gid} ({j}/{len(remaining_list)})")
            details = scraper.get_sku_details(gid)
            
            if details is not None:
                # æŠ“å–æˆåŠŸï¼šåŠ å…¥ Session ä¸¦è¨˜éŒ„ ID
                st.session_state.collected_data.extend(details)
                st.session_state.processed_gids.add(gid)
            else:
                st.error(f"âŒ å•†å“ {gid} æŠ“å–è¶…æ™‚ï¼Œå·²è·³éï¼Œç¨å¾Œå¯é‡æ–°åŸ·è¡ŒçºŒå‚³ã€‚")
            
            progress_bar.progress(j / len(remaining_list))
            time.sleep(5) # 5ç§’å†·å»
            
    # é¡¯ç¤ºç›®å‰å·²æŠ“åˆ°çš„æ‰€æœ‰è³‡æ–™
    if st.session_state.collected_data:
        result_df = pd.DataFrame(st.session_state.collected_data)
        st.write(f"### ç›®å‰å·²ç´¯è¨ˆè³‡æ–™ï¼š{len(result_df)} ç­†")
        st.dataframe(result_df.head(50))
        
        # ä¸‹è¼‰æŒ‰éˆ•
        csv_btn = result_df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è¼‰ç›®å‰å·²å®Œæˆè³‡æ–™", csv_btn, file_name="ä¸€æ‰‹æ–·é»å‚™ä»½.csv", mime="text/csv")
        
        # å¦‚æœå…¨éƒ¨è·‘å®Œï¼Œè§¸ç™¼è‡ªå‹•ä¸‹è¼‰
        if len(st.session_state.processed_gids) >= len(total_list):
            f_auto_download(result_df)
            st.balloons()